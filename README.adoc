= crisp-mini-wrangler

Crisp Take Home Assignment - Mini CSV Wrangler

== Usage

=== Configuration

Transformation configurations define ...

* ... the columns in the input file and their expected order,
* ... fields in the output,
* ... the transformation from one or more input columns to an output field
* ... the output serialization formats

.Configuration Example
```
transformation {
    input {"Order Number","Year","Month","Day"}
    record {
        field("OrderID", IntegerField("Order Number"))
        field("OrderDate", DateField("Year","Month","Day"))
        field("OrderUnit", ConstantField("kg))
    }
    output {
        records {
            format = "$OrderID,$OrderDate,$OrderUnit\n"
            stdout
            file("./output.csv")
        }
        errors {
             stderr
         }
    }
}
```

== Build

**Requirements**

* Java/JVM >= 11
* Kotlin >= 1.3.50

== Architecture

=== Terminology

* `Column` refers to a column in the input CSV
* `Row` is a a single line from the CSV as an array of Strings
* `Record` refers to to the typed and transformed representation of of a `Row`
* A `column ref` of a `Field` conceptually points to a `Column` from the input CSV
* `Field` represents a typed and named value in a `Record` footnote:[So a String from a `Column` is to `Row` as `Field` is to `Record`]
* The `TransformationConfig` holds information about the input `Columns` to process and how to aggregate them into `Fields`
* `unmarshalling` refers to the process of getting a typed `Field` value from a `Row`

== Assumptions

. [[a-1]]Every input CSV file contains only records of one type, i.e. record-based text formats are not supported
. [[a-2]]Every input CSV file contains exactly one header row designating the contained record's fields
. [[a-3]]The order of records in the output may differ from that in the input.
Assuming an analytics data ingestion context, this should be fine.
. [[a-4]]Field types do not need to be specified within the external DSL, we can provide a pool of field types and transformations and configure the system using these
. [[a-5]]We don't need a rich domain model for the CSV data, as it is bound to differ per input format
. [[a-6]] We can't use an internal (Kotlin) DSL, as the requirements explicitly state otherwise.
I assume that the mappings are not created by core developers but rather analysts/consultants, potentially from customer's staff.
. [[a-7]]While ideally the CSV parser used supports different encodings transparently, this application assumes the input to be in UTF-8
. [[a-8]]TODO: Which Decimal Formats?
. [[a-9]]The target data format does not need to be specified externally, as the requirements state that
+
[quote]
____
 [...] use case is taking a delimited data file from a customer and massaging it to fit with a standardized schema [...]
____

. [[a-10]]For proper casing of product names, we assume that each word is capitalized.
Truecasing product/brand names would require a dictionary containing properly cased names.
Truecasing of is an interesting NLP problem in itself, but I consider it outside the scope of this assignment.
footnote:[I once built a (pretty specific) true casing tool that scraped existing data from a product catalog website, put the words into an Aspell dictionary and checked/corrected all uppercase product names against it.
Random pointers, just for reference:
tRuEcasIng paper: http://delivery.acm.org/10.1145/1080000/1075116/p152-lita.pdf?ip=95.91.254.50&id=1075116&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1567268373_f3b20cc8a2b5af687c026079519d467d, A Rust implementation: https://github.com/despawnerer/truecase, A writeup on truecasing methods: https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21
]
. [[a-11]] I assume all input dates in UTC. Supporting additional timezones would require extending the configuration by the timezone of dates represented as `String` and the corresponding type unmarshalling mechanism.
I consider this out of scope for now.
. [[a-12]] FIXME: Decimal format representation
. [[a-13]] The transformation to an output record does not require data from multiple input rows.

== Architecture Decision Records

=== [[adr-1]]ADR 1: Stream Based Architecture

**Status:** proposed

**Decision**

Since the requirements state that the input files may be potentially very large, the application should be able to deal with potentially unbounded streams of records.

**Consequences**

=== [[adr-2]]ADR 2: Transformations are run asynchronously

**Status:** proposed

**Context**

Transformations can be simple and fast for basic text wrangling, but can also grow complex and even have the need to access external systems. E.g. if an output field is required to contain the date of the maximum shelf live, this information could be required to be retrieved from a master data system of some sort.
Another example would be to take shipping times or opening hours of a store into consideration for date calculations.

**Decision**

Since the order of records does not matter for our purposes (see <<a-3>>), we can run transformations in an async fashion.
It must be able to turn off async behavior, since, if the transformation is cheap, processing in parallel may well be faster.
We'll need to run a performance comparison later on.

```
Stream<Record> -> Dispatch to Transformer Worker -> Transform -> Stream<Record>

```

**Consequences**

* Costly transformations can be performed in parallel.
* The system is able to transform rows either in parallel or sequentially
* The order of output records is not guaranteed if processed in parallel (see <<a-3>>)

Several (though not very sophisticated) test runs w/ 1000, 10_000 and 100_000 rows and different (mocked) transformation durations on a 8 core i7 2015 MBP indicated that:

* If transformations are instantaneous, there is no real difference between sequential processing is significantly faster
* For transformations requiring 10ms and more, are roughly 8 times faster.
This is consistent with the number of cores in the test machine.

The following table contains the rough average from tables generated by `LearningTests.compare sync and async processing`

.Measurements for different transformation durations
|===
|Rows|Transformation ms|Duration sync|Duration async|async/sync
|100|0|7ms|121ms|17
|100|10|1150ms|150ms|0.13
|100|100|10s|1.4s|0.14
|1000|0|15ms|118ms|0.12
|1000|10|11s|1.4s|0.12
|1000|100|102s|13s|7.8
|10000|0|98ms|542ms|5.5
|10000|10|1.8min|15s|0.11
|10000|100|17min|2.15min|0.14
|100000|0|796ms|4262ms|5.3
|100000|10|19.7min|2.7min|0.13
|100000|100|2.85h|21.5min|0.12
|===

For more detailed analysis, I'd set up a https://openjdk.java.net/projects/code-tools/jmh/[JMH] benchmark, but I'll skip that for now.

**Addendum after implementation**

Actual measurements w/ generated test data show that for the example transformation from the instructions, the performance does not benefit from parallelization.
See `TransformerPerformanceTests`

.Measurements for example transformation
|===
|Rows|Duration sync ms|Duration async ms|async/sync
|100|68|62|0.9117647058823529
|1000|50|228|4.56
|10000|185|509|2.7513513513513512
|100000|283|2775|9.80565371024735
|1000000|2640|30180|11.431818181818182
|===

=== [[adr-3]]ADR 3: CSV Parser

**Status:** proposed

**Context**

While implementing a CSV parser by simply splitting rows at a delimiter character seems simple at the first glance, there are a lot of things that actually need to be taken into consideration (escaping delimiters in text columns, text delimiting, line breaks in texts, different line separators etc.).

For the JVM, a lot of CSV parser libraries are available, though some of which are quite dated.
Univocity, a supplier of commercial data ingestion products, provides a https://github.com/uniVocity/csv-parsers-comparison#jdk-8[performance comparison].

When selecting a parser, we need to make sure that it can perform in a streaming fashion as not to break <<adr-1>>.

We don't need advanced mapping to objects (as we'll deal with multiple formats as opposed to having a rich domain model), as we will provide and run our own transformations on the parsed data, only robust and fast async parsing of CSV records.

**Decision**

We're using https://simpleflatmapper.org/0101-getting-started-csv.html[SimpleFlatMapper].

The SimpleFlatMapper CSV module is the fastest OSS parser in the comparison mentioned above.
It is actively being developed, with ~20 releases in 2019 so far and 300 stars on github.

SFM supports callback, iterator and stream based parsing.

Detailed performance stats by the SFM team https://simpleflatmapper.org/12-csv-performance.html[here].

We're using the raw parser flavor as not to ...

* ... tie our implementation to much into a parser implementation
* ... introduce runtime overhead for object mapping


**Consequences**

=== [[adr-4]]ADR 4: Decouple configuration data and configuration DSL

**Status:** proposed

**Decision**

**Consequences**

=== [[adr-5]]ADR 5: Don't put transformation code into DSL

**Status:** proposed

**Decision**

**Consequences**

=== [[adr-6]]ADR 6: Don't use infix functions in the record definition DSL

**Context**

Infix functions for record definitions would allow for writing sth. like `field "foo" from "foo col" asType string` which would be quite readable.
However, since https://kotlinlang.org/docs/reference/functions.html#infix-notation[infix functions can only have a single parameter], we'd have to


**Status:** proposed

**Decision**

**Consequences**

=== [[adr-7]]ADR 7: Minimize coupling w/ CSV parser

**Status:** proposed

**Decision**

**Consequences**

=== [[adr-n]]ADR n:

**Status:** proposed

**Decision**

**Consequences**

== Next Steps

* Try returning a stream instead of using callbacks for result (and possibly error) handlers as an alternative
* Improve DSL validation
* Refactor Tests to use parameterized tests, e.g. for field validations
* Support transformations for `Records` and factor out `StringField`  concatenations, product name casing and `StaticStringValueField` (support all types of static values)
* Introduce own exception hierarchy instead of using (only) stock exceptions.
* Support better parsing for dates, perhaps merge `DateField` and `FormattedDateField`
* Generally make a better distinction between field types and transformations
