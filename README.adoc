= crisp-mini-wrangler

Crisp Take Home Assignment - Mini CSV Wrangler

== Run

=== Configuration

Transformation configurations define ...

* ... the columns in the input file and their expected order,
* ... fields in the output,
* ... the transformation from one or more input columns to an output field
* ... the output serialization formats

.Configuration Example
```
Transformation {
    input {"Order Number","Year","Month","Day"}
    record {
        field("OrderID", IntegerField("Order Number"))
        field("OrderDate", DateField("Year","Month","Day"))
        field("OrderUnit", ConstantField("kg))
    }
    output {
        records {
            format = "$OrderID,$OrderDate,$OrderUnit\n"
            stdout
            file("./output.csv")
        }
        errors {
             stderr
         }
    }
}
```
== Assumptions

. [[a-1]]Every input CSV file contains only records of one type, i.e. record-based text formats are not supported
. [[a-2]]Every input CSV file contains exactly one header row designating the contained record's fields
. [[a-3]]The order of records in the output may differ from that in the input. Assuming an analytics data ingestion context, this should be fine.
. [[a-4]]Field types do not need to be specified within the external DSL, we can provide a pool of field types and transformations and configure the system using these
. [[a-5]]We don't need a rich domain model for the CSV data, as it is bound to differ per input format
. [[a-6]]We can't use an internal (Kotlin) DSL, as the requirements explicitly state otherwise. I assume that the mappings are not
created by core developers but rather analysts/consultants, potentially from customer's staff.
. [[a-7]]While ideally the CSV parser used supports different encodings, this application assumes the input to be in UTF-8
. [[a-8]]TODO: Which Decimal Formats?
. [[a-9]]The target data format does not need to be specified externally, as the requirements state that
+
[quote]
____
 [...] use case is taking a delimited data file from a customer and massaging it to fit with a standardized schema [...]
____
. [[a-10]]For proper casing of product names, we assume that each word is capitalized.  Truecasing product/brand names would require a dictionary containing properly cased names.
Truecasing of is an interesting NLP problem in itself, but I consider it outside the scope of this assignment.
footnote:[I once built a (pretty specific) true casing tool that scraped existing data from a product catalog website, put the words into an Aspell dictionary
and checked/corrected all uppercase product names against it. Random pointers, just for reference:
tRuEcasIng paper: http://delivery.acm.org/10.1145/1080000/1075116/p152-lita.pdf?ip=95.91.254.50&id=1075116&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1567268373_f3b20cc8a2b5af687c026079519d467d,
A Rust implementation: https://github.com/despawnerer/truecase,
A writeup on truecasing methods: https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21
]
. [[a-11]] I assume all input dates in UTC. Supporting additional timezones would require extending the configuration
by the timezone of dates represented as `String` and the corresponding type unmarshalling mechanism. I consider this out
of scope for now.
. [[a-12]] FIXME: Decimal format representation
. [[a-13]] The transformation to an output record does not require data from multiple input rows.

== Architecture Decision Records

=== [[adr-1]]ADR 1: Stream Based Architecture

**Status:** proposed

**Decision**

Since the requirements state that the input files may be potentially very large, the application will be able
to deal with potentially unbounded streams of records.

**Consequences**

=== [[adr-2]]ADR 2: Transformations are run asynchronously

**Status:** proposed

**Context**

Transformations can be simple and fast for basic text wrangling, but can also grow complex and even
have the need to access external systems. E.g. if an output field is required to contain the date of the
maximum shelf live, this information could be required to be retrieved from a master data system of some
sort. Another example would be to take shipping times or opening hours of a store into consideration for date
calculations.

**Decision**

Since the order of records does not matter for our purposes (see <<a-3>>), we can run transformations
in an async fashion.
It must be able to turn off async behavior, since, if the transformation is cheap, processing in parallel may well be faster.
We'll need to run a performance comparison later on.

```
Stream<Record> -> Dispatch to Transformer Worker -> Transform -> Stream<Record>

```

**Consequences**

* Costly transformations can be performed in parallel.
* The system is able to transform rows either in parallel or sequentially
* The order of output records is not guaranteed if processed in parallel (see <<a-3>>)

Several (though not very sophisticated) test runs w/ 1000, 10_000 and 100_000 rows and
different (mocked) transformation durations on a 8 core i7 2015 MBP indicated that:

* If transformations are instantaneous, there is no real difference between sequential processing is significantly faster
* For transformations requiring 10ms and more, are roughly 8 times faster. This is consistent with the number of cores in the test machine.

The following table contains the rough average from tables generated by `LearningTests.compare sync and async processing`

|===
|Rows|Transformation ms|Duration sync|Duration async|async Factor
|100|0|7ms|121ms|17
|100|10|1150ms|150ms|0.13
|100|100|10s|1.4s|0.14
|1000|0|15ms|118ms|0.12
|1000|10|11s|1.4s|0.12
|1000|100|102s|13s|7.8
|10000|0|98ms|542ms|5.5
|10000|10|1.8min|15s|0.11
|10000|100|17min|2.15min|0.14
|100000|0|796ms|4262ms|5.3
|100000|10|19.7min|2.7min|0.13
|100000|100|2.85h|21.5min|0.12
|===

For more detailed analysis, I'd set up a https://openjdk.java.net/projects/code-tools/jmh/[JMH] benchmark, but I'll skip that for now.


=== [[adr-3]]ADR 3: CSV Parser

**Status:** proposed

**Context**

While implementing a CSV parser by simply splitting rows at a delimiter character seems
simple at the first glance, there are a lot of things that actually need to be taken into
consideration (escaping delimiters in text columns, text delimiting, line breaks in texts, different line separators etc.).

For the JVM, a lot of CSV parser libraries are available, though some of which are quite dated. Univocity, a supplier of commercial
data ingestion products, provides a https://github.com/uniVocity/csv-parsers-comparison#jdk-8[performance comparison].

When selecting a parser, we need to make sure that it can perform in a streaming fashion as not to break <<adr-1>>.

We don't need advanced mapping to objects (as we'll deal with multiple formats as opposed to having a rich domain model),
as we will provide and run our own transformations on the parsed data, only robust and fast async parsing of CSV records.

**Decision**

We're using https://simpleflatmapper.org/0101-getting-started-csv.html[SimpleFlatMapper].

The SimpleFlatMapper CSV module is the fastest OSS parser in the comparison mentioned above.
It is actively being developed, with ~20 releases in 2019 so far and 300 stars on github.

SFM supports callback, iterator and stream based parsing.

Detailed performance stats by the SFM team https://simpleflatmapper.org/12-csv-performance.html[here].

We're using the raw parser flavor as not to ...

* ... tie our implementation to much into a parser implementation
* ... introduce runtime overhead for object mapping


**Consequences**


== Next Steps

